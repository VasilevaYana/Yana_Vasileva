{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Краткое описание решения\n",
        "\n",
        "В данной задаче необходимо было построить прогнозный интервал цен (`price_p05`, `price_p95`). Основная сложность заключалась в метрике **IoU (Intersection over Union)**, которая плохо оптимизируется стандартными функциями потерь (MSE, MAE, Quantile Loss).\n",
        "\n",
        "**Мой подход:**\n",
        "1.  **Архитектура:** Полносвязная нейронная сеть (MLP) на PyTorch.\n",
        "2.  **Функция потерь:** Реализован кастомный **Soft-IoU Loss** — дифференцируемая аппроксимация метрики соревнования. Это позволило модели напрямую максимизировать пересечение интервалов.\n",
        "3.  **Признаки:** Использованы агрегированные статистики по категориям (Target Encoding), PCA для погоды и циклические признаки дат.\n",
        "4.  **Валидация:** 5-Fold Cross-Validation.\n",
        "5.  **Post-Processing:** Небольшое расширение предсказанного интервала на 5% (`width * 1.05`), что компенсирует неуверенность модели и штрафы метрики за \"непопадание\".\n",
        "\n",
        "**Результат:**\n",
        "*   **Public Score:** 0.2713\n",
        "*   **Private Score:** 0.2637\n"
      ],
      "metadata": {
        "id": "29Y58T0fVBsz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Atuwn0VxVAZQ",
        "outputId": "01b0ea18-78c9-432e-d170-411cf39a3d9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import KFold\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Фиксация random seed для воспроизводимости\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "SEED = 42\n",
        "seed_everything(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Загрузка данных и Feature Engineering\n",
        "\n",
        "Мы создаем признаки, описывающие как временные зависимости (сезонность), так и ценовые характеристики товаров внутри их категорий.\n",
        "\n",
        "Ключевые моменты:\n",
        "*   **Log-target:** Цены логарифмируются (`log1p`), чтобы стабилизировать дисперсию.\n",
        "*   **Weather PCA:** Погодные признаки сжимаются в одну компоненту.\n",
        "*   **Target Encoding:** Для каждого товара добавляются средние цены его категории (чтобы решить проблему \"холодного старта\" для новых товаров в тесте)."
      ],
      "metadata": {
        "id": "1BShzwviViBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Пути к файлам\n",
        "train = pd.read_csv('/content/train.csv')\n",
        "test = pd.read_csv('/content/test.csv')\n",
        "submission = pd.read_csv('/content/sample_submission.csv')\n",
        "\n",
        "\n",
        "# 2. Преобразование дат и объединение\n",
        "train['dt'] = pd.to_datetime(train['dt'])\n",
        "test['dt'] = pd.to_datetime(test['dt'])\n",
        "train = train.sort_values('dt').reset_index(drop=True)\n",
        "\n",
        "train['is_train'] = 1\n",
        "test['is_train'] = 0\n",
        "\n",
        "# Заглушки для цен в тесте\n",
        "test['price_p05'] = np.nan\n",
        "test['price_p95'] = np.nan\n",
        "\n",
        "df = pd.concat([train, test], ignore_index=True, sort=False)\n",
        "\n",
        "# 3. Базовый препроцессинг (Погода + Даты)\n",
        "# PCA для погоды\n",
        "weather_cols = ['precpt', 'avg_temperature', 'avg_humidity', 'avg_wind_level']\n",
        "df[weather_cols] = df[weather_cols].fillna(df[weather_cols].mean())\n",
        "pca = PCA(n_components=1)\n",
        "df['weather_pca'] = pca.fit_transform(StandardScaler().fit_transform(df[weather_cols]))\n",
        "\n",
        "# Циклические признаки даты\n",
        "df['month_sin'] = np.sin(2 * np.pi * df['dt'].dt.month / 12)\n",
        "df['month_cos'] = np.cos(2 * np.pi * df['dt'].dt.month / 12)\n",
        "df['dow_sin'] = np.sin(2 * np.pi * df['dt'].dt.dayofweek / 7)\n",
        "df['dow_cos'] = np.cos(2 * np.pi * df['dt'].dt.dayofweek / 7)\n",
        "\n",
        "# 4. Target Encoding (Средние цены по категориям)\n",
        "# Считаем логарифмы таргетов\n",
        "df['log_p05'] = np.log1p(df['price_p05'])\n",
        "df['log_p95'] = np.log1p(df['price_p95'])\n",
        "\n",
        "# Считаем статистики ТОЛЬКО по train части, чтобы избежать data leakage\n",
        "stats_cols = []\n",
        "for col in ['second_category_id', 'third_category_id']:\n",
        "    mapper = df[df['is_train'] == 1].groupby(col)[['log_p05', 'log_p95']].agg(['mean', 'std'])\n",
        "    mapper.columns = [f'{col}_mean_p05', f'{col}_std_p05', f'{col}_mean_p95', f'{col}_std_p95']\n",
        "    df = df.merge(mapper, on=col, how='left')\n",
        "    stats_cols.extend(mapper.columns.tolist())\n",
        "\n",
        "# Статистики по самому товару (для тех, что есть в трейне)\n",
        "product_mapper = df[df['is_train'] == 1].groupby('product_id')[['log_p05', 'log_p95']].agg(['mean', 'std'])\n",
        "product_mapper.columns = ['product_mean_p05', 'product_std_p05', 'product_mean_p95', 'product_std_p95']\n",
        "df = df.merge(product_mapper, on='product_id', how='left')\n",
        "stats_cols.extend(product_mapper.columns.tolist())\n",
        "\n",
        "# Заполнение пропусков (для новых товаров берем среднее по категории)\n",
        "for col in stats_cols:\n",
        "    df[col] = df[col].fillna(df[col].mean())\n",
        "\n",
        "if 'cluster_id' not in df.columns:\n",
        "    df['cluster_id'] = 0\n",
        "\n",
        "print(f\"Размер объединенного датасета: {df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyFOQdQiVjFK",
        "outputId": "30e2329a-f52c-47c6-c75a-7b865ebe0b78"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер объединенного датасета: (57150, 41)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Подготовка данных для PyTorch\n",
        "\n",
        "Мы используем `StandardScaler` для числовых признаков и `OneHotEncoder` для категориальных.\n",
        "Целевые переменные преобразуются в два значения:\n",
        "1.  **Midpoint:** Центр интервала `(log_p95 + log_p05) / 2`\n",
        "2.  **Width:** Ширина интервала `(log_p95 - log_p05)`\n",
        "\n",
        "Это позволяет нейросети предсказывать геометрические параметры интервала."
      ],
      "metadata": {
        "id": "XTb2CBfBVqs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Отбор признаков\n",
        "num_features = [\n",
        "    'n_stores', 'precpt', 'avg_temperature', 'avg_humidity', 'avg_wind_level',\n",
        "    'weather_pca', 'month_sin', 'month_cos', 'dow_sin', 'dow_cos',\n",
        "    'second_category_id_mean_p05', 'second_category_id_mean_p95',\n",
        "    'third_category_id_mean_p05', 'third_category_id_mean_p95',\n",
        "    'product_mean_p05', 'product_std_p05', 'product_mean_p95', 'product_std_p95'\n",
        "]\n",
        "cat_features = ['activity_flag', 'holiday_flag', 'cluster_id']\n",
        "\n",
        "# Разделение\n",
        "train_df = df[df['is_train'] == 1].copy()\n",
        "test_df = df[df['is_train'] == 0].copy()\n",
        "\n",
        "# Таргеты (Log Space)\n",
        "y_p05 = train_df['log_p05'].values.astype(np.float32)\n",
        "y_p95 = train_df['log_p95'].values.astype(np.float32)\n",
        "\n",
        "# Преобразуем в Midpoint и Width\n",
        "y_mid = (y_p05 + y_p95) / 2\n",
        "y_width = (y_p95 - y_p05)\n",
        "y_target = np.stack([y_mid, y_width], axis=1)\n",
        "\n",
        "# Пайплайн предобработки\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features)\n",
        "    ])\n",
        "\n",
        "X_train_scaled = preprocessor.fit_transform(train_df[num_features + cat_features]).astype(np.float32)\n",
        "X_test_scaled = preprocessor.transform(test_df[num_features + cat_features]).astype(np.float32)\n",
        "\n",
        "print(f\"Train shape: {X_train_scaled.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUAV5psxVuXF",
        "outputId": "731120bc-0685-4d6d-91a0-6cb3e0be864b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (29100, 23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Модель и Custom Loss\n",
        "\n",
        "Главный секрет успеха. Вместо того чтобы минимизировать MSE (ошибку в значениях), мы максимизируем IoU.\n",
        "Так как IoU — дискретная функция, мы используем ее сглаженную версию **Soft-IoU**.\n",
        "\n",
        "$$ \\text{Loss} = 1 - \\text{mean}\\left(\\frac{\\text{Intersection}}{\\text{Union}}\\right) $$\n",
        "\n",
        "Ширина интервала (`width`) предсказывается через функцию активации `Softplus`, чтобы гарантировать положительные значения."
      ],
      "metadata": {
        "id": "kN0KBeDpWBAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PricingDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32) if y is not None else None\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.X[idx], self.y[idx]) if self.y is not None else self.X[idx]\n",
        "\n",
        "class IntervalNet(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(IntervalNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2) # Output: [Midpoint, Raw_Width]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        mid = out[:, 0]\n",
        "        # Softplus гарантирует width > 0\n",
        "        width = torch.nn.functional.softplus(out[:, 1])\n",
        "        return mid, width\n",
        "\n",
        "class SoftIoULoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SoftIoULoss, self).__init__()\n",
        "\n",
        "    def forward(self, pred_mid, pred_width, target_mid, target_width):\n",
        "        # Восстанавливаем границы\n",
        "        pred_low = pred_mid - pred_width / 2\n",
        "        pred_high = pred_mid + pred_width / 2\n",
        "        target_low = target_mid - target_width / 2\n",
        "        target_high = target_mid + target_width / 2\n",
        "\n",
        "        # Пересечение\n",
        "        inter_low = torch.max(pred_low, target_low)\n",
        "        inter_high = torch.min(pred_high, target_high)\n",
        "        intersection = torch.clamp(inter_high - inter_low, min=0)\n",
        "\n",
        "        # Объединение\n",
        "        pred_area = pred_high - pred_low\n",
        "        target_area = target_high - target_low\n",
        "        union = pred_area + target_area - intersection + 1e-6\n",
        "\n",
        "        iou = intersection / union\n",
        "        return 1.0 - torch.mean(iou)"
      ],
      "metadata": {
        "id": "i3IppMr0V0tx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Обучение (5-Fold Cross Validation)\n",
        "\n",
        "Мы обучаем ансамбль из 5 моделей на разных разбиениях данных. Это повышает стабильность и дает прирост метрики.\n",
        "В каждом фолде сохраняется модель с лучшим Val IoU."
      ],
      "metadata": {
        "id": "qAisXTCbV9j7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_FOLDS = 5\n",
        "EPOCHS = 40\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Накопители для предсказаний на тесте\n",
        "test_preds_mid_sum = np.zeros(len(X_test_scaled))\n",
        "test_preds_width_sum = np.zeros(len(X_test_scaled))\n",
        "\n",
        "kfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "print(f\"Starting 5-Fold Training on {DEVICE}...\")\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_scaled)):\n",
        "    print(f\"\\n=== FOLD {fold+1}/{N_FOLDS} ===\")\n",
        "\n",
        "    # Подготовка данных\n",
        "    train_ds = PricingDataset(X_train_scaled[train_idx], y_target[train_idx])\n",
        "    val_ds = PricingDataset(X_train_scaled[val_idx], y_target[val_idx])\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Инициализация модели\n",
        "    model = IntervalNet(input_dim=X_train_scaled.shape[1]).to(DEVICE)\n",
        "    criterion = SoftIoULoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    best_loss = 1.0\n",
        "    best_state = None\n",
        "\n",
        "    # Цикл обучения\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        for X_b, y_b in train_loader:\n",
        "            X_b, y_b = X_b.to(DEVICE), y_b.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            mid, width = model(X_b)\n",
        "            # y_b[:, 0] is Mid, y_b[:, 1] is Width\n",
        "            loss = criterion(mid, width, y_b[:, 0], y_b[:, 1])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Валидация\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for X_b, y_b in val_loader:\n",
        "                X_b, y_b = X_b.to(DEVICE), y_b.to(DEVICE)\n",
        "                mid, width = model(X_b)\n",
        "                val_loss += criterion(mid, width, y_b[:, 0], y_b[:, 1]).item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            best_state = model.state_dict()\n",
        "\n",
        "    print(f\"Best Val IoU: {1.0 - best_loss:.5f}\")\n",
        "\n",
        "    # Предсказание на тесте лучшей моделью фолда\n",
        "    model.load_state_dict(best_state)\n",
        "    model.eval()\n",
        "    temp_mids, temp_widths = [], []\n",
        "    test_loader = DataLoader(PricingDataset(X_test_scaled), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_b in test_loader:\n",
        "            X_b = X_b.to(DEVICE)\n",
        "            mid, width = model(X_b)\n",
        "            temp_mids.append(mid.cpu().numpy())\n",
        "            temp_widths.append(width.cpu().numpy())\n",
        "\n",
        "    test_preds_mid_sum += np.concatenate(temp_mids)\n",
        "    test_preds_width_sum += np.concatenate(temp_widths)\n",
        "\n",
        "print(\"\\nTraining Finished!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48fks2HXV8Up",
        "outputId": "e4223918-4e4e-44cf-a0af-a4a986c61831"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting 5-Fold Training on cuda...\n",
            "\n",
            "=== FOLD 1/5 ===\n",
            "Best Val IoU: 0.29798\n",
            "\n",
            "=== FOLD 2/5 ===\n",
            "Best Val IoU: 0.29715\n",
            "\n",
            "=== FOLD 3/5 ===\n",
            "Best Val IoU: 0.29654\n",
            "\n",
            "=== FOLD 4/5 ===\n",
            "Best Val IoU: 0.29137\n",
            "\n",
            "=== FOLD 5/5 ===\n",
            "Best Val IoU: 0.30001\n",
            "\n",
            "Training Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Формирование сабмишна и Post-Processing\n",
        "\n",
        "После усреднения предсказаний ансамбля мы применяем важный трюк: **расширение интервала**.\n",
        "Метрика IoU сильно штрафует, если истинная цена хотя бы немного выходит за границы предсказанного интервала. Умножение ширины на коэффициент `1.05` (5%) значительно повышает стабильность на Leaderboard."
      ],
      "metadata": {
        "id": "wcO82k_pWK9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Усредняем предсказания 5 фолдов\n",
        "avg_mid_log = test_preds_mid_sum / N_FOLDS\n",
        "avg_width_log = test_preds_width_sum / N_FOLDS\n",
        "\n",
        "# === Post-Processing ===\n",
        "# Увеличиваем ширину на 5%, чтобы \"поймать\" граничные случаи\n",
        "WIDTH_MULTIPLIER = 1.05\n",
        "avg_width_log = avg_width_log * WIDTH_MULTIPLIER\n",
        "\n",
        "# Восстанавливаем цены из логарифмов\n",
        "final_p05 = np.expm1(avg_mid_log - avg_width_log / 2)\n",
        "final_p95 = np.expm1(avg_mid_log + avg_width_log / 2)\n",
        "\n",
        "# Гарантия неотрицательности\n",
        "final_p05 = np.maximum(0, final_p05)\n",
        "final_p95 = np.maximum(0, final_p95)\n",
        "\n",
        "# Сохранение\n",
        "submission_ids = test_df['row_id'].values\n",
        "sub = pd.DataFrame({\n",
        "    'row_id': submission_ids,\n",
        "    'price_p05': final_p05,\n",
        "    'price_p95': final_p95\n",
        "})\n",
        "sub = sub.sort_values('row_id')\n",
        "\n",
        "sub.to_csv('submission_solution.csv', index=False)\n",
        "print(\"Файл submission_solution.csv успешно сохранен.\")\n",
        "print(sub.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkLTj-SHWIk2",
        "outputId": "acc7b0a3-bb67-4976-b83d-62a6d429012a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Файл submission_solution.csv успешно сохранен.\n",
            "   row_id  price_p05  price_p95\n",
            "0     0.0   0.950212   1.164189\n",
            "1     1.0   0.907801   1.157708\n",
            "2     2.0   0.907185   1.157771\n",
            "3     3.0   0.912715   1.160802\n",
            "4     4.0   0.906945   1.157113\n"
          ]
        }
      ]
    }
  ]
}